{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training ends\n",
            "Episode 0 Episodic Reward 1.0 Maximum Reward 1.0 EPSILON 1\n",
            "Stoppedat eisode 99!\n"
          ]
        }
      ],
      "source": [
        "from gym.wrappers import AtariPreprocessing, FrameStack\n",
        "import keras \n",
        "from keras import layers\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "import gym as gym\n",
        "import random\n",
        "random.seed(2212)\n",
        "np.random.seed(2212)\n",
        "tf.random.set_seed(2212)\n",
        "from collections import deque\n",
        "input_shape = [2]\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "EPISODES = 560\n",
        "REPLAY_MEMORY_SIZE = 1_00_000\n",
        "MINIMUM_REPLAY_MEMORY = 1_000\n",
        "MINIBATCH_SIZE = 32\n",
        "EPSILON = 1\n",
        "EPSILON_DECAY = 0.99\n",
        "MINIMUM_EPSILON = 0.001\n",
        "DISCOUNT = 0.99\n",
        "ENV_NAME = 'BreakoutNoFrameskip-v4'\n",
        "EPSILON_RANDOM_FRAMES = 50000\n",
        "MAX_STEPS_PER_EPISODE = 100\n",
        "UPDATE_AFTER_ACTIONS = 4\n",
        "UPDATE_TARGET_NETWROK = 10000\n",
        "MAX_EPISODES = 10\n",
        "\n",
        "\n",
        "# Environment details\n",
        "env = gym.make(ENV_NAME)\n",
        "env = AtariPreprocessing(env)\n",
        "env = FrameStack(env,4)\n",
        "# action_dim = env.action_space.n\n",
        "# observation_dim = env.observation_space.shape\n",
        "\n",
        "# defining the input shape and the number of outputs from the environment\n",
        "input_shape = env.observation_space.shape  # Assuming self.observation_dim is a tuple\n",
        "n_outputs = env.action_space.n\n",
        "\n",
        "# Creating a simple sequential neural network model with 3 layers (400, 300, 2)\n",
        "# model = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Dense(400, activation='relu', input_shape=input_shape),\n",
        "#     tf.keras.layers.Dense(300, activation='relu'),\n",
        "#     tf.keras.layers.Dense(n_outputs, activation='linear')\n",
        "# ])\n",
        "\n",
        "num_actions = 4\n",
        "def create_dueling_q_model():\n",
        "    # Input layer (already adjusting to 'channels last')\n",
        "    input_layer = layers.Input(shape=(84, 84, 4))\n",
        "    \n",
        "    # Convolutional layers\n",
        "    x = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(input_layer)\n",
        "    x = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(x)\n",
        "    x = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(x)\n",
        "    \n",
        "    # Flattening the last convolutional output\n",
        "    x = layers.Flatten()(x)\n",
        "    \n",
        "    # Dueling streams\n",
        "    # State value tower - V(s)\n",
        "    state_value = layers.Dense(512, activation='relu')(x)\n",
        "    state_value = layers.Dense(1, activation=None)(state_value)\n",
        "    \n",
        "    # Action advantage tower - A(s, a)\n",
        "    action_advantage = layers.Dense(512, activation='relu')(x)\n",
        "    action_advantage = layers.Dense(num_actions, activation=None)(action_advantage)\n",
        "    \n",
        "    # Combine the state and action advantage to get the final Q-values\n",
        "    # Q(s, a) = V(s) + (A(s, a) - mean(A(s, a)))\n",
        "    q_values = layers.Lambda(lambda values: values[0] + (values[1] - tf.reduce_mean(values[1], axis=1, keepdims=True)),\n",
        "                             output_shape=(num_actions,))([state_value, action_advantage])\n",
        "\n",
        "    # Build the model\n",
        "    model = keras.Model(inputs=input_layer, outputs=q_values)\n",
        "    return model\n",
        "\n",
        "model = create_dueling_q_model()\n",
        "target_model1 = create_dueling_q_model()\n",
        "\n",
        "# Compiling the neural network using Mean Square Error as loss function and\n",
        "model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.005))\n",
        "\n",
        "# Cloning the model to create a target model\n",
        "# target_model1 = tf.keras.models.clone_model(model)\n",
        "\n",
        "from collections import deque\n",
        "# Replay memory to store experiances of the model with the environment\n",
        "replay_buffer = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
        "\n",
        "'''Defining epsilon greedy policy function to determine an action\n",
        "to decide whether to explore or exploit the knowledge acquired'''\n",
        "def epsilon_greedy_policy(state, epsilon, frame_count):\n",
        "    if np.random.uniform(0, 1) < epsilon or frame_count < EPSILON_RANDOM_FRAMES:\n",
        "        action = np.random.randint(0, num_actions)\n",
        "        return action\n",
        "    else:\n",
        "        ''' Look into Notebook'''\n",
        "        # $# action = np.argmax(model.predict(np.expand_dims(state, axis=0), verbose=0)[0])\n",
        "        state_tensor = keras.ops.convert_to_tensor(state)\n",
        "        state_tensor = keras.ops.expand_dims(state_tensor, 0)\n",
        "        action_probs = model(state_tensor, training=False)\n",
        "        # Take best action\n",
        "        action = keras.ops.argmax(action_probs[0]).numpy()\n",
        "\n",
        "        return action\n",
        "\n",
        "'''This function samples a batch of experiences from the replay buffer for training the model.'''\n",
        "def sample_experiences(batch_size):\n",
        "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
        "    batch = [replay_buffer[index] for index in indices]\n",
        "    return [\n",
        "        np.array([experience[field_index] for experience in batch])\n",
        "        for field_index in range(5)\n",
        "    ] \n",
        "\n",
        "max_reward = -999999\n",
        "'''This function defines the steps the agent has to take based on the epsilon greedy policy\n",
        " and accumulates the reward'''\n",
        "def play_one_step(env, state, epsilon, frame_count):\n",
        "    # defining reward acquired in each episode\n",
        "    global episode_reward\n",
        "    # Calling epsilon greedy function to select action\n",
        "    action = epsilon_greedy_policy(state, epsilon, frame_count)\n",
        "    # Executing the action and returns 4 values\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    # $$# next_state = np.array(next_state)\n",
        "    # accumulating rewards in each step of the epsiode(max=-200)\n",
        "    episode_reward += reward\n",
        "    # truncating if the car reaches the flag on the mountain top within 200 steps\n",
        "    # if done and episode_length < 200:\n",
        "    #     # If episode is ended then we have won the game. Defining Stratagem to make the agent learn faster\n",
        "    #     reward = 250 + episode_reward\n",
        "    #     # save the model weights if we get a epsiode reward greater than maximum reward defined\n",
        "    #     if(episode_reward > max_reward):\n",
        "    #         print(\"Saving the model with reward\", episode_reward)\n",
        "    #         model.save_weights(\"agent\"+str(episode_reward)+\"_agent_.weights.h5\")\n",
        "    # else:\n",
        "    #     # In oher cases reward will be proportional to the distance that car has travelled\n",
        "    #     # from it's previous location + velocity of the car\n",
        "    #     reward = 5*abs(next_state[0] - state[0]) + 3*abs(state[1])\n",
        "\n",
        "    # append this result to the replay buffer(experince replay)\n",
        "    replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    return next_state, action, episode_reward, max_reward, done, info\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "rewards = []\n",
        "best_score = 0\n",
        "\n",
        "batch_size = 32\n",
        "discount_factor = 0.95\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-1)\n",
        "loss_fn = tf.keras.losses.mean_squared_error\n",
        "\n",
        "def training_step(batch_size):\n",
        "\n",
        "    # Randomly sample a batch of experiences from the replay buffer\n",
        "    minibatch = random.sample(replay_buffer, MINIBATCH_SIZE)\n",
        "    # Initialize lists to store the current and next states\n",
        "    X_cur_states = []\n",
        "    X_next_states = []\n",
        "\n",
        "    # Populate the current and next states lists with data from the sampled experiences\n",
        "    for index, sample in enumerate(minibatch):\n",
        "        cur_state, action, reward, next_state, done = sample\n",
        "        \n",
        "        X_cur_states.append(cur_state)\n",
        "        X_next_states.append(next_state)\n",
        "\n",
        "    # Convert the lists to arrays\n",
        "    \n",
        "\n",
        "    X_cur_states = np.array(X_cur_states)\n",
        "    X_next_states = np.array(X_next_states)\n",
        "    X_cur_states = X_cur_states.transpose(0, 2, 3, 1)\n",
        "    X_next_states = X_next_states.transpose(0, 2, 3, 1)\n",
        "    \n",
        "    # Initialize an array to store the Q values for the current state-action pairs\n",
        "    cur_action_values = model.predict(X_cur_states, verbose =0)\n",
        "\n",
        "    # Use the target model to predict the Q values for the next states\n",
        "    next_action_values = target_model1.predict(X_next_states, verbose=0)\n",
        "    \n",
        "    '''Update the Q values for the current state-action pairs based on the rewards \n",
        "       and the maximum predicted Q values for the next states'''\n",
        "    for index, sample in enumerate(minibatch):\n",
        "        cur_state, action, reward, next_state, done = sample\n",
        "        if not done:\n",
        "            # if episode is not done then we have to calculate the Q value using Bellman's equation\n",
        "            # Q(st, at) = rt + DISCOUNT * max(Q(s(t+1), a(t+1)))\n",
        "            cur_action_values[index][action] = reward + DISCOUNT * np.amax(next_action_values[index])\n",
        "        else:\n",
        "            # if episode is done then Q value will be the reward\n",
        "            # Q(st, at) = rt\n",
        "            cur_action_values[index][action] = reward\n",
        "            \n",
        "    # train the agent with new Q values for the states and the actions\n",
        "    model.fit(X_cur_states, cur_action_values, verbose=0)\n",
        "    \n",
        "# Initialize lists to store Q-values, positions, velocities, and actions taken\n",
        "q_values = []\n",
        "positions = []\n",
        "velocities = []\n",
        "actiion_taken = []\n",
        "frame_count =0\n",
        "\n",
        "# Loop through the episodes\n",
        "for episode in range(600):\n",
        "    # Reset the environment for the state to be reset\n",
        "    state = env.reset()\n",
        "    state = np.array(state) # $$# Will see for env.reset\n",
        "    # done = False\n",
        "    episode_reward = 0\n",
        "    episode_length = 0\n",
        "\n",
        "    # Run through different sets of steps untill 200 steps are completed or the car reaches the flag\n",
        "    for timestep in range(1,MAX_STEPS_PER_EPISODE):\n",
        "        frame_count += 1\n",
        "        # Increment the episode length\n",
        "        episode_length += 1\n",
        "\n",
        "        # Play one step in the environment and get the next state, action, reward, max reward, done status, and info\n",
        "        next_state, action, reward, max_reward, done, info  = play_one_step(env, state, EPSILON, frame_count)\n",
        "        # Append the position, velocity, and action to their respective lists\n",
        "        # positions.append(next_state[0])\n",
        "        # velocities.append(next_state[1]) # $$# mgight need it later\n",
        "        # actiion_taken.append(action)\n",
        "\n",
        "        # Update the current state to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # If the replay buffer is not large enough, skip the training step\n",
        "        if(len(replay_buffer) < MINIMUM_REPLAY_MEMORY):\n",
        "            continue\n",
        "        elif (frame_count % UPDATE_AFTER_ACTIONS == 0):\n",
        "            training_step(batch_size)\n",
        "\n",
        "    print(\"training ends\")\n",
        "\n",
        "    # Update the maximum reward if the current reward is greater\n",
        "    max_reward = max(reward, max_reward)\n",
        "\n",
        "    # Print the episode number, episodic reward, maximum reward, and epsilon\n",
        "    print('Episode', episode, 'Episodic Reward', reward, 'Maximum Reward', max_reward, 'EPSILON', EPSILON)\n",
        "    rewards.append(reward)\n",
        "    state = np.array([(state)])\n",
        "    state= state.transpose(0, 2, 3, 1)\n",
        "    # Append the mean Q-value of the current state to the Q-values list\n",
        "    q_values.append(np.mean(model.predict(state, verbose=0)[0]))\n",
        "    \n",
        "    # Every 20 episodes, update the weights of the target model with the weights of the training model\n",
        "    if frame_count % UPDATE_TARGET_NETWROK == 0:\n",
        "        target_model1.set_weights(model.get_weights())\n",
        "    \n",
        "    # If epsilon is greater than the minimum epsilon and the replay buffer is large enough, decay epsilon\n",
        "    if(EPSILON > MINIMUM_EPSILON and len(replay_buffer) > MINIMUM_REPLAY_MEMORY):\n",
        "        EPSILON *= EPSILON_DECAY\n",
        "\n",
        "    if episode_length >= MAX_EPISODES:\n",
        "        print(\"Stoppedat eisode {}!\".format(episode_length))\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://github.com/SergeyGasparyan/ai-for-game-playing/blob/main/src/breakout_game.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WsX4zKCo1EcI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.1.0 (SDL 2.0.16, Python 3.10.0)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ],
      "source": [
        "# import pygame as pg\n",
        "\n",
        "# SCREEN_SIZE = 424, 430\n",
        "\n",
        "# BRICK_HEIGHT, BRICK_WIDTH = 13, 32\n",
        "\n",
        "# PADDLE_HEIGHT, PADDLE_WIDTH = 8, 50\n",
        "# PADDLE_Y = SCREEN_SIZE[1] - PADDLE_HEIGHT - 10\n",
        "# MAX_PADDLE_X = SCREEN_SIZE[0] - PADDLE_WIDTH\n",
        "\n",
        "# BALL_DIAMETER = 12\n",
        "# BALL_RADIUS = BALL_DIAMETER // 2\n",
        "# MAX_BALL_X = SCREEN_SIZE[0] - BALL_DIAMETER\n",
        "# MAX_BALL_Y = SCREEN_SIZE[1] - BALL_DIAMETER\n",
        "\n",
        "# NUM_BRICKS_VERTICAL = 11\n",
        "# NUM_BRICKS_HORIZONTAL = 12\n",
        "\n",
        "# BLACK = (0, 0, 0)\n",
        "# WHITE = (255, 255, 255)\n",
        "# BLUE = (0, 0, 255)\n",
        "# PADDLE_COLOR = (129, 133, 137)\n",
        "# BRICK_COLOR = (153, 255, 204)\n",
        "\n",
        "# FPS = 60\n",
        "\n",
        "# pg.init()\n",
        "# screen = pg.display.set_mode(SCREEN_SIZE)\n",
        "# pg.display.set_caption(\"Breakout - Atari\")\n",
        "# clock = pg.time.Clock()\n",
        "\n",
        "\n",
        "# class Breakout:\n",
        "#     def __init__(self, vel=12):\n",
        "#         self.capture = 0\n",
        "#         self.vel = vel\n",
        "#         self.ball_vel = [vel, -vel]\n",
        "#         self.reward = 0.1\n",
        "#         self.terminal = False\n",
        "\n",
        "#         self.paddle = pg.Rect(\n",
        "#             SCREEN_SIZE[1] // 2, PADDLE_Y, PADDLE_WIDTH, PADDLE_HEIGHT\n",
        "#         )\n",
        "#         self.ball = pg.Rect(\n",
        "#             SCREEN_SIZE[1] // 2 + 10,\n",
        "#             PADDLE_Y - BALL_DIAMETER,\n",
        "#             BALL_DIAMETER,\n",
        "#             BALL_DIAMETER,\n",
        "#         )\n",
        "#         self.create_bricks()\n",
        "\n",
        "#     def create_bricks(self):\n",
        "#         self.bricks = []\n",
        "#         y_ofs = 20\n",
        "#         for _ in range(NUM_BRICKS_VERTICAL):\n",
        "#             x_ofs = 15\n",
        "\n",
        "#             for _ in range(NUM_BRICKS_HORIZONTAL):\n",
        "#                 self.bricks.append(pg.Rect(x_ofs, y_ofs, BRICK_WIDTH, BRICK_HEIGHT))\n",
        "#                 x_ofs += BRICK_WIDTH + 1\n",
        "\n",
        "#             y_ofs += BRICK_HEIGHT + 1\n",
        "\n",
        "#     def draw_bricks(self):\n",
        "#         for i, brick in enumerate(self.bricks):\n",
        "#             pg.draw.rect(screen, BRICK_COLOR, brick)\n",
        "\n",
        "#     def draw_paddle(self):\n",
        "#         pg.draw.rect(screen, PADDLE_COLOR, self.paddle)\n",
        "\n",
        "#     def draw_ball(self):\n",
        "#         pg.draw.circle(\n",
        "#             screen,\n",
        "#             WHITE,\n",
        "#             (self.ball.left + BALL_RADIUS, self.ball.top + BALL_RADIUS),\n",
        "#             BALL_RADIUS,\n",
        "#         )\n",
        "\n",
        "#     def check_input(self, input_action):\n",
        "#         # 0: LEFT, 1: Right\n",
        "#         if input_action[0] == 1:\n",
        "#             self.paddle.left -= self.vel\n",
        "#             if self.paddle.left < 0:\n",
        "#                 self.paddle.left = 0\n",
        "\n",
        "#         elif input_action[1] == 1:\n",
        "#             self.paddle.left += self.vel\n",
        "#             if self.paddle.left > MAX_PADDLE_X:\n",
        "#                 self.paddle.left = MAX_PADDLE_X\n",
        "\n",
        "#     def move_ball(self):\n",
        "#         self.ball.left += self.ball_vel[0]\n",
        "#         self.ball.top += self.ball_vel[1]\n",
        "\n",
        "#         if self.ball.left <= 0:\n",
        "#             self.ball.left = 0\n",
        "#             self.ball_vel[0] = -self.ball_vel[0]\n",
        "#         elif self.ball.left >= MAX_BALL_X:\n",
        "#             self.ball.left = MAX_BALL_X\n",
        "#             self.ball_vel[0] = -self.ball_vel[0]\n",
        "\n",
        "#         if self.ball.top < 0:\n",
        "#             self.ball.top = 0\n",
        "#             self.ball_vel[1] = -self.ball_vel[1]\n",
        "#         elif self.ball.top >= MAX_BALL_Y:\n",
        "#             self.ball.top = MAX_BALL_Y\n",
        "#             self.ball_vel[1] = -self.ball_vel[1]\n",
        "\n",
        "#     def take_action(self, input_action):\n",
        "#         pg.event.pump()\n",
        "#         for event in pg.event.get():\n",
        "#             if event.type == pg.QUIT:\n",
        "#                 pg.quit()\n",
        "#                 quit()\n",
        "\n",
        "#         screen.fill(BLACK)\n",
        "#         self.check_input(input_action)\n",
        "#         self.move_ball()\n",
        "\n",
        "#         # Handle Collisions\n",
        "#         for brick in self.bricks:\n",
        "#             if self.ball.colliderect(brick):\n",
        "#                 self.reward = 2\n",
        "#                 self.ball_vel[1] = -self.ball_vel[1]\n",
        "#                 self.bricks.remove(brick)\n",
        "#                 break\n",
        "\n",
        "#         if len(self.bricks) == 0:\n",
        "#             self.terminal = True\n",
        "#             self.__init__()\n",
        "\n",
        "#         if self.ball.colliderect(self.paddle):\n",
        "#             self.ball.top = PADDLE_Y - BALL_DIAMETER\n",
        "#             self.ball_vel[1] = -self.ball_vel[1]\n",
        "#         elif self.ball.top > self.paddle.top:\n",
        "#             self.terminal = True\n",
        "#             self.__init__()\n",
        "#             self.reward = -2\n",
        "\n",
        "#         self.draw_bricks()\n",
        "#         self.draw_ball()\n",
        "#         self.draw_paddle()\n",
        "\n",
        "#         image_data = pg.surfarray.array3d(pg.display.get_surface())\n",
        "\n",
        "#         pg.display.update()\n",
        "#         clock.tick(FPS)\n",
        "\n",
        "#         return image_data, self.reward, self.terminal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bqoVrvhE1SBE"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        self.number_of_actions = 2\n",
        "        self.gamma = 0.99\n",
        "        self.final_epsilon = 0.05\n",
        "        self.initial_epsilon = 0.1\n",
        "        self.number_of_iterations = 2000000\n",
        "        self.replay_memory_size = 750000\n",
        "        self.minibatch_size = 32\n",
        "        self.explore = 3000000\n",
        "\n",
        "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
        "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
        "        self.fc5 = nn.Linear(512, self.number_of_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
        "\n",
        "        return self.fc5(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-7ygsHXK1W0u"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'utils'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# import Breakout\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device, preprocessing\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NeuralNetwork\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(model):\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# import Breakout\n",
        "from utils import device, preprocessing\n",
        "from models import NeuralNetwork\n",
        "\n",
        "\n",
        "def test(model):\n",
        "    game_state = Breakout()\n",
        "\n",
        "    # initial action is do nothing\n",
        "    action = torch.zeros([model.number_of_actions], dtype=torch.float32).to(device)\n",
        "    action[0] = 1\n",
        "    image_data, _, _ = game_state.take_action(action)\n",
        "    image_data = preprocessing(image_data)\n",
        "    state = torch.cat((image_data, image_data, image_data, image_data)).unsqueeze(0)\n",
        "\n",
        "    while True:\n",
        "        # get output from the neural network\n",
        "        output = model(state)[0]\n",
        "        action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "\n",
        "        # get action\n",
        "        action_index = torch.argmax(output)\n",
        "        action[action_index] = 1\n",
        "\n",
        "        # get next state\n",
        "        image_data_1, _, _ = game_state.take_action(action)\n",
        "        image_data_1 = preprocessing(image_data_1)\n",
        "        state_1 = torch.cat((state.squeeze(0)[1:, :, :], image_data_1)).unsqueeze(0)\n",
        "\n",
        "        # set state to be state_1\n",
        "        state = state_1\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = NeuralNetwork()\n",
        "    model.load_state_dict(torch.load('model_weights/best.pth').state_dict())\n",
        "    test(model.eval().to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn0Hupoa1ZpB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "from models import NeuralNetwork\n",
        "from breakout_game import Breakout\n",
        "from utils import device, init_weights, preprocessing\n",
        "\n",
        "\n",
        "def train(model, start, lr=0.0003, optim_name='adam', loss_name='mse', num_iter_save=1000):\n",
        "    # define optimizer\n",
        "    if optim_name == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    elif optim_name == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    # initialize error loss\n",
        "    if loss_name == 'mse':\n",
        "        criterion = nn.MSELoss()\n",
        "    elif loss_name == 'crossentropy':\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # instantiate game\n",
        "    game_state = Breakout()\n",
        "\n",
        "    # initialize replay memory\n",
        "    replay_buffer = deque()\n",
        "\n",
        "    # initial action\n",
        "    action = torch.zeros([model.number_of_actions], dtype=torch.float32).to(device)\n",
        "    image_data, reward, terminal = game_state.take_action(action)\n",
        "    image_data = preprocessing(image_data)\n",
        "    state = torch.cat((image_data, image_data, image_data, image_data)).unsqueeze(0)\n",
        "\n",
        "    # initialize epsilon value\n",
        "    epsilon = model.initial_epsilon\n",
        "    iteration = 0\n",
        "\n",
        "    # main infinite loop\n",
        "    while iteration < model.number_of_iterations:\n",
        "        # get output from the neural network\n",
        "        output = model(state)[0] #Â Output size = torch.Size([2]) tensor([-0.0278,  1.7244]\n",
        "        #output = model(state)\n",
        "\n",
        "        # initialize action\n",
        "        action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "\n",
        "        # epsilon greedy exploration\n",
        "        random_action = random.random() <= epsilon\n",
        "\n",
        "        # Pick action --> random or index of maximum q value\n",
        "        action_index = [torch.randint(model.number_of_actions, torch.Size([]), dtype=torch.int)\n",
        "                        if random_action\n",
        "                        else torch.argmax(output)][0].to(device)\n",
        "\n",
        "        action[action_index] = 1\n",
        "\n",
        "        if epsilon > model.final_epsilon:\n",
        "            epsilon -= (model.initial_epsilon - model.final_epsilon) / model.explore\n",
        "\n",
        "        # get next state and reward\n",
        "        image_data_1, reward, terminal = game_state.take_action(action)\n",
        "        image_data_1 = preprocessing(image_data_1)\n",
        "\n",
        "        state_1 = torch.cat((state.squeeze(0)[1:, :, :], image_data_1)).unsqueeze(0)\n",
        "        action = action.unsqueeze(0)\n",
        "        reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0).to(device)\n",
        "\n",
        "        # save transition to replay memory\n",
        "        replay_buffer.append((state, action, reward, state_1, terminal))\n",
        "\n",
        "        # if replay memory is full, remove the oldest transition\n",
        "        if len(replay_buffer) > model.replay_memory_size:\n",
        "            replay_buffer.popleft()\n",
        "\n",
        "        # sample random mini_batch\n",
        "        # it picks k unique random elements, a sample, from a sequence: random.sample(population, k)\n",
        "        mini_batch = random.sample(replay_buffer, min(len(replay_buffer), model.minibatch_size))\n",
        "\n",
        "        # unpack mini_batch\n",
        "        state_batch   = torch.cat(tuple(d[0] for d in mini_batch)).to(device)\n",
        "        action_batch  = torch.cat(tuple(d[1] for d in mini_batch)).to(device)\n",
        "        reward_batch  = torch.cat(tuple(d[2] for d in mini_batch)).to(device)\n",
        "        state_1_batch = torch.cat(tuple(d[3] for d in mini_batch)).to(device)\n",
        "\n",
        "        # get output for the next state\n",
        "        output_1_batch = model(state_1_batch)\n",
        "\n",
        "        # set y_j to r_j for terminal state, otherwise to r_j + gamma*max(Q) Target Q value Bellman equation.\n",
        "        y_batch = torch.cat(tuple(reward_batch[i] if mini_batch[i][4]\n",
        "                                  else reward_batch[i] + model.gamma * torch.max(output_1_batch[i])\n",
        "                                  for i in range(len(mini_batch))))\n",
        "\n",
        "        q_value = torch.sum(model(state_batch) * action_batch, dim=1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_batch = y_batch.detach()\n",
        "\n",
        "        # calculate loss\n",
        "        loss = criterion(q_value, y_batch)\n",
        "\n",
        "        # do backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # set state to be state_1\n",
        "        state = state_1\n",
        "        iteration += 1\n",
        "\n",
        "        if iteration % num_iter_save == 0:\n",
        "            print('Saving the model')\n",
        "            torch.save(model, f\"model_weights/model_{iteration}.pth\")\n",
        "\n",
        "        elapsed_time = time.time() - start\n",
        "        elapsed_minutes = int(elapsed_time // 60)\n",
        "        elapsed_seconds = int(elapsed_time % 60)\n",
        "\n",
        "        print(f\"Iteration: {iteration} Elapsed time: {elapsed_minutes}:{elapsed_seconds} epsilon: {epsilon:.5f} action: {action_index.cpu().item()} Reward: {reward.cpu().numpy()[0][0]:.1f}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--lr', type=float, default=0.0003, help=\"learning rate\")\n",
        "    parser.add_argument('--optim', type=str, default='adam', help=\"optimzer: adam, or sgd\")\n",
        "    parser.add_argument('--loss', type=str, default='mse', help=\"loss function: mse, or cross_entropy\")\n",
        "    parser.add_argument('--num_iter_save', type=int, default=1000, help=\"number of iterations to save the model checkpoint\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    os.makedirs('model_weights/', exist_ok=True)\n",
        "    model = NeuralNetwork()\n",
        "    if os.path.exists('model_weights/best.pth'):\n",
        "        model = torch.load('model_weights/best.pth')\n",
        "    else:\n",
        "        model.apply(init_weights)\n",
        "\n",
        "    model = model.train().to(device)\n",
        "\n",
        "    start = time.time()\n",
        "    train(model, start, args.lr, args.optim, args.loss, args.num_iter_save)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aavw9LSx1cvh"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def preprocessing(image):\n",
        "\timage_data = cv2.cvtColor(cv2.resize(image, (84, 84)), cv2.COLOR_BGR2GRAY)\n",
        "\timage_data[image_data > 0] = 255\n",
        "\timage_data = np.reshape(image_data,(84, 84, 1))\n",
        "\timage_tensor = image_data.transpose(2, 0, 1)\n",
        "\timage_tensor = image_tensor.astype(np.float32)\n",
        "\timage_tensor = torch.from_numpy(image_tensor).to(device)\n",
        "\n",
        "\treturn image_tensor\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
        "        torch.nn.init.uniform(m.weight, -0.01, 0.01)\n",
        "        m.bias.data.fill_(0.01)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
